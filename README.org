#+TITLE: Project for finding a Bioregionalisation around Australia
* Run Locally
:PROPERTIES:
:ID:       org:cdc364db-b138-472f-97b8-7563acab7407
:END:
Check out the latest commit.
Currently the ~f_targets~ branch is the latest.
# Copyright 2017-2024 Philip Dyer
# SPDX-License-Identifier: CC-BY-4.0

Set the root directory, here we use home for simplicity:

#+begin_src sh
export ROOTDIR=~
cd $ROOTDIR
#+end_src



#+begin_src sh
git clone -b f_targets --single-branch https://github.com/mathmarecol/pdyer_aus_bio.git
cd pdyer_aus_bio
#+end_src

Modify the paths in ~./shell/aus_bio_local_submit.sh~ to point to real folders on your system.

Make sure you have a working R installation.
If you are using nix, then you can get the exact development environment I use. run:

#+begin_src sh
cd $ROOTDIR
git clone https://github.com/PhDyellow/nix-shell-pin.git
git clone https://github.com/PhDyellow/nix_r_dev_shell.git
chmod +x nix-shell-pin/nix-shell-pin.sh
cd nix_r_dev_shell
# /vmshare/cust-nix/nix_shell_pin.sh r_shell.nix ~/.nixshellgc #Not contained in the repo, I use it to pin builds so gc doesn't lead to slow rebuilds.
../nix-shell-pin/nix-shell-pin.sh r_shell.nix ~/.nixshellgc
cd $ROOTDIR
#+end_src

Then to run the project:

#+begin_src sh
cd $ROOTDIR/pdyer_aus_bio/shell
GIT_BRANCH=f_targets_quick WORKERS=2 ./aus_bio_local_submit.sh
#+end_src

* Run on UQ RCC HPC
:PROPERTIES:
:ID:       org:18abd131-c324-4b9e-b395-3256b18c62c4
:END:

First, build the singularity image.
See [[id:org:08572e87-3196-4198-a161-18df33698dc1][*.nix]] below for how to build it.
Admin rights are probably required, so don't try to do it on the HPC.

Move the singularity image to the HPC cluster.
~aus_bio_module.lua~ assumes that you will put it in a scratch location, currently ~/30days/uqpdyer/r-singularity-aus-bio.img~.

#+begin_src sh
rsync -irc -L --append-verify ~/r-singularity-aus-bio.img awoonga:/30days/uqpdyer/r-singularity-aus-bio.img
#+end_src

Now check out the latest commit, anywhere on the HPC should work.

#+begin_src sh
git clone -b f_targets --single-branch https://github.com/MathMarEcol/pdyer_aus_bio.git ~/aus_bio
#+end_src

Modify the paths in ~./code/shell/aus_bio_pbs_submit.sh~ to point to real folders in the HPC.
Also modify the job params if needed.
Change ~aus_bio_module.lua~ to match the scratch location where the singularity image is placed.


Then to run the project:

#+begin_src sh
cd ~/aus_bio/shell
qsub -v GIT_BRANCH=f_targets_quick,WORKERS=8  aus_bio_pbs_submit.sh
#+end_src

* Run on SMP Getafix
:PROPERTIES:
:ID:       org:42a50f1e-f6dd-4642-b0a0-65d629987dba
:END:

First, build the singularity image.
See [[id:org:08572e87-3196-4198-a161-18df33698dc1][*.nix]] below for how to build it.
Admin rights are probably required, so don't try to do it on the HPC.

Move the singularity image to the HPC cluster.
~aus_bio_module.lua~ assumes that you will put it in a scratch location, currently
~/data/uqpdyer/resources/hpc_scratch/r-singularity-aus-bio.img~.

#+begin_src sh
rsync -irc -L --append-verify ~/r-singularity-aus-bio.img getafix:/data/uqpdyer/resources/hpc_scratch/r-singularity-aus-bio.img
#+end_src

Now check out the latest commit, anywhere on the HPC should work.

#+begin_src sh
git clone https://github.com/MathMarEcol/pdyer_aus_bio.git /data/uqpdyer/projects/aus_bio_ch3/code
#+end_src

Modify the paths in ~./code/shell/aus_bio_slurm_submit.sh~ to point to real folders in the HPC.
Also modify the job params if needed.
Change ~aus_bio_module_slurm.lua~ to match the scratch location where the singularity image is placed.


Then to run the project:

#+begin_src sh
cd /data/uqpdyer/projects/aus_bio_ch3/code/shell
sbatch --export=GIT_BRANCH=f_targets_quick,WORKERS=8  aus_bio_slurm_submit.sh
#+end_src


** Things that are fragile
:PROPERTIES:
:ID:       org:e7f66cdd-d7b5-42ce-ba5c-fff7ac4378b9
:END:

Clustermq is run from inside a singularity container, and the singularity container is NOT aware of the different workload managers (slurm, pbs, etc.).
The solution involves ssh'ing into the login node from the job and submitting a new job from there, see https://mschubert.github.io/clustermq/articles/userguide.html#running-master-inside-containers

Some clusters need proxy settings to access external data.
Setting the env variable ~https_proxy~ works for many cli commands:
#+begin_src  sh
https_proxy="http://its-ri-proxy01.hpc.dc.uq.edu.au:3128"
#+end_src

Git accepts a global proxy setting, run this once per HPC:
#+begin_src sh
git config --global http.proxy  http://its-ri-proxy01.hpc.dc.uq.edu.au:3128
#+end_src


The ~aus_bio_module_*.lua~ files need specific SINGULARITY_BIND setups for each HPC, and in some cases uses my username.

Make sure all the paths are correct

* Build project
:PROPERTIES:
:ID:       org:a5ee9fee-f00d-435e-b85e-85bb6e0428b8
:END:
#+begin_src sh
  module load use.own
  module load aus_bio_module.lua #assumes you have copied this to ~/privatemodules/
  cd code/
  Rscript drake_plan.R #Rscript is an alias to call the singularity container and run Rscript from there.
#+end_src
* Components
:PROPERTIES:
:ID:       org:9141685d-4154-42ec-91b5-287b5d201f43
:END:
** aus_bio_module.lua
:PROPERTIES:
:ID:       org:ee2fa59e-b577-4112-9a10-9c89fa7e52f1
:END:
Mostly a convenience script, but may be needed for HPC.
Defines a module.
To use:
#+begin_src
module load use.own
module load aus_bio_module.lua
#+end_src
Now the singularity module is loaded, and the container specifically for this project can be access by using ~R~ and ~Rscript~.
** The .img file
:PROPERTIES:
:ID:       org:4c36674c-46fe-40fd-997d-19edcc893d7b
:END:
The .img file is a singularity container.
It will have some very long name like this:

~r-singularity-aus_bio.img~

The .img file contains a specific version of R, R packages and system libraries.

The purpose is easy addition of packages, even if they are not supported by HPC, and reproducibility.
Later on I can give the .img file and the source code, and they will get the same results.

** *.nix
:PROPERTIES:
:ID:       org:08572e87-3196-4198-a161-18df33698dc1
:END:

The project ~r-dev-shell~ contains the R environment.

https://github.com/PhDyellow/nix_r_dev_shell

All the *.nix files (all_packages.nix, r_packages.nix, singularity_image.nix and r_shell.nix) are files for the Nix package manager, and are the definitive way to reproduce the environment.

Using the package manager Nix, you can rebuild the .sif file with the following code:
      #+begin_src sh
        nix build -f apptainer_image.nix -o /tmp/r-apptainer-aus-bio.sif  #runs on "apptainer_image.nix" in current folder
      #+end_src

The end result is guaranteed to be the exact image I generated with the same commands.

** _targets.R
:PROPERTIES:
:ID:       org:f3d9a7ad-3801-42fb-b165-3c812b2778bc
:END:
The master R script that builds the project.

Run with:
#+begin_src sh
Rscript -e "targets::tar_make_clustermq(workers=2)"
#+end_src
* The singularity .sif files are large, and are not in the repo
:PROPERTIES:
:ID:       org:76b953b3-3016-497e-bb02-f354644e7903
:END:
The *.sif files, which contain R and all relevant packages and system software, are synced via ~scp~ or ~rsync~.
* Directory structure
:PROPERTIES:
:ID:       org:09e255e4-a92d-439c-b959-6b998e00880f
:END:

The whole project is assumed to be inside the MathMarEcol QRIScloud collection ~Q1216/pdyer~.
The

The ~code/~ folder contains the drake_plan.R and other scripts and code for the project.

The data are all stored in a different QRIScloud collection, ~Q1215~.
Different HPC systems have a different folder for the QRIScloud data, but Q1215 and Q1216 are always sibling folders, so relative paths will work, and will be more reliable than hard paths.

Given that HPC code should not be run over the network, I copy the relevant parts of ~Q1215~ and ~Q1216~ into ~30days~ or something similar on Awoonga, before running ~Rscript drake_plan.R~
* Future Layers
:PROPERTIES:
:ID:       org:bf84771c-bf91-4a29-976f-42d927d98f10
:END:
Future layers are handled by BioORACLE v2.2, as well as the ~env_year~ and ~env_pathway~ params.
* Update for targets and crew

Crew provides a unified frontend for workers.

No longer need to differentiate between local and cluster execution, or call a different top-level function depending on whether future, clustermq or sequential execution are needed.
Always call ~tar_make()~ and ensure the ~controllers~ tar_option is set appropriately.

** Balancing workloads

Each target has a distinct resource requirement.

Some are small and fast, some require lots of memory, some internally use paralellisation, and benefit from having lots of cores available.

Experience tells me that it is better to compute targets sequentially rather than in parallel if the total runtime is the same.
Parallel computation should only be used if there are spare resources.

In practice, this means that branches that internally run in parallel should be given the whole node.

- Branch types
	- single :: single cpu, can run in parallel with other branches
	- GPU :: needs the GPU, or a whole node for BLAS/LAPACK
		- BLAS may need the env var ~XXX_NUM_THREADS~ set, according to the number of CPUs
	- multicore :: the branch internally uses parallel, so can use a whole node
		- Need to make sure future is configured

RAM requirements are set per job, 4GB is enough for many small jobs.
Bigger jobs will need tuning according to the dataset, can use 100's of GB.



** Making sure the right controllers are used

One goal is to make the code run in different environments with minimal changes.

Crew helps, but different controllers are needed for different environments, eg. local vs slurm.

I may end up needing to use the configure_parallel function to just list controllers, and use some flag to choose between them.



** Future framework

Targets will use crew to assign branches to workers.

Some functions can run in parallel, but all use the future framework to decide if it is possible.

crew might be able to set up future plans for workers that expect multicore operations.
It doesn't seem to.
Each target could set the plan just before calling the function.
Given that the resoureces are specified in the same place, the relevant information would be kept together.


future.callr is probably the most flexible and reliable for running within a single node.
future.mirai is under development, but locally it behaves largely like future.callr.
